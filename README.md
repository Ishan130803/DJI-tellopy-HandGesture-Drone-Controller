# Hello Guys,

In this project, I will use a pretrained model (trained on my own dataset) to control the dji-tello drone movement using hand gestures. 


# Frameworks/Dependencies
This project is build on the following frameworks:
1. Mediapipe
2. OpenCv
3. tensorflow (only to import the model and get prediction out of it)
4. NumPy (essential dependency)
5. djitellopy (API to control the dji tello drone)

### Installing the Dependencies
1. Type cmd in start searh of windows and open the command prompt
2. paste the following commands one by one
   pip install tensorflow
   pip install numpy
   pip install opencv-python
   pip install mediapipe
   pip install djitellopy
3. Dependencies will be installed automactically

I am using python 3.8.6 by the way and it works smoothly here


# Working
The model present in /models/gesture_recognition.h5 is trained to identify following gestures.
|Index No.|Gesture|Label|
|:----|:-|:-|
|0|Palm open but, middle finger and ring finger are curled inside|backward|
|1|Thumbs Down| down|
|2|Index finger pointing in upward direction|flip|
|3| Palm open|forward|
|4| Middle finger pointing in upward direction|land|
|5|Thumb left|left|
|6|Thumb right|right|
|7|Thumbs Up |up|

Model takes mean and variance normalized landmark data generated by mediapipe hand detection API. So, remember to preprocess the data before you begin working with the mdoel for your own project

Pretrained model is quite robust. It will correctly identify the hand gestures accurately even hand gestures are significantly deviated. For example, in thumbs up gesture, it will correctly, identify that gesture even if it is not exactly pointing in upward direction. 

You can use this model to repurpose for your own project

* NOTE : These gesture are only valid for right hand *

# Possible Movements

### Basic Movements
|S.No.|Gesture|Movement|
|:--|:--|:--|
|1.|Thumbs Up|Upward Drone Movements|
|2.|Thumbs Down|Downward Drone Movement|
|3.|Thumbs Left|Left Drone Movement|
|4.|Thumbs Right|Right Drone Movement|
|5.|Palm front open|Forward Drone Movement|
|6.|Palm front open but, middle and ring finger curled|Backward Drone Movement|
|7.|Middle Finger Pointing up|Land|
### Executing flips
|S.No.|Gesture|Movement|
|:--|:--|:--|
|8.|Index Finger pointing Up then, Thumbs left|Left flip|
|9.|Index finger pointing Up then, Thumbs Right|Right flip|
|10.|Index finger pointing Up then, forward drone movement gesture|Forward Flip|
|11.|Index finger pointing Up then. backward drone movement gesture|Back Flip|
### Special Command 
|S.No.|Gesture|Movement|
|:--|:--|:--|
|12.|Index Finger pointing Up then quickly moving hand outside the view of camera|This will make the drone halt anticipating next instruction but, you can use this to halt the drone quickly as it may be difficult to quickly move hand away from frame unprompted|


* NOTE : Right now, I have set the main().tello to None and commented all the tello related commands so that you can get a feel of how well this works. You will get the status of the drone on the screen in realtime *

# Prediction and Drone Status indicator
|Prediction|This field shows the prediction of the hand gesture by the model|
|:--|:--|
|Drone Status|This field shows the status of the drone. Drone status will be different than prediction becuse you need to safeguard the drone from erratic and quick commands|

# Starting the Drone

### Initialization
1. Before starting, you must install the frameworks using the commnds mentioned in the Frameworks/dependencies section. Then, launch app.py
2. Wait for few seconds until the webcam stream pops up.
3. You will see the Drone Status : IDLE and Prediction : NaN.
4. To make the drone takeoff, you must perform forward movement gesture. Any other gesture will cause no change in drone's movement however, it will be detected by the app and will be shown with the 'Prediction' field.
5. Perform forward movement gesture. Drone Status will show the progress of drone taking off.

### Executing Basic Movements
1. Refer to the table in the section "Possible Movements".
2. Perform various gestures and see the command updated in the 'Drone Status'

### Landing
1. Show the middle finger for a few seconds
2. When drone takes the input, it will be updated in Drone Status.
3. After taking the command, you will not be able to give any further commands until the drone has landed. However, your gesture prediction will work just fine and it will be updated accordingly
4. After drone has landed, you have essentially returned to square one (drone can re-takeoff if you perform forward movement gesture and other gesture movement will not have any impact on drone)

### Re-takeoff
1. After your drone has landed, you can takeoff again by performing the forward movement gesture

### Turning off the drone
 At any moment, you can press the q to quit the window and drone will land automactically if not landed already


 # Thank You



    
